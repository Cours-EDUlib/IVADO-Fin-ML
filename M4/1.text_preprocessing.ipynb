{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1126GM_j68_8bGZb57hHIwmcxIFQ_gTli","timestamp":1642371846871},{"file_id":"1eufmQNK_uJg7wGE3mucQKuSDqsvGsWMd","timestamp":1636818566483}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"98FpJ6zEGIWq"},"source":["## Natural Language Processing Pipelines (NLP Pipelines)"]},{"cell_type":"markdown","metadata":{"id":"lSXKMATMGNbx"},"source":["NLP algorithms are based on machine learning algorithms. Doing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things."]},{"cell_type":"markdown","metadata":{"id":"8bLKvuD6GomW"},"source":["![picture](https://drive.google.com/uc?export=view&id=16e6wwg2eKxwwZgQ2DOfY65SEtpdX8Kv-)\n"]},{"cell_type":"markdown","source":["In this tutorial, you’ll learn:\n","\n","    How to apply pre-processing techniques\n","    How to apply text-normalization techniques\n","    How to use spaCy and nltk\n"],"metadata":{"id":"dbpn8BQg_l2a"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfPQO1QtiLYo","executionInfo":{"status":"ok","timestamp":1674165756437,"user_tz":300,"elapsed":2693,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"6f862e58-7104-4d3a-b4c2-ac25e9b3efbe"},"source":["!pip install unidecode"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.8/dist-packages (1.3.6)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Apc0Sigvt5SN","executionInfo":{"status":"ok","timestamp":1674165760142,"user_tz":300,"elapsed":3710,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"c3b75080-d3d6-476a-9d2e-983b0ffd77a8"},"source":["!pip install contractions"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: contractions in /usr/local/lib/python3.8/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.8/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8gMyOcKevKlm","executionInfo":{"status":"ok","timestamp":1674165763894,"user_tz":300,"elapsed":3756,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"32ec393e-ce35-4cd5-ae69-174ba729f9af"},"source":["!pip install word2number"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: word2number in /usr/local/lib/python3.8/dist-packages (1.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bXLqY8Rrrepj"},"source":["## Import all needed libraries\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"bgl-BdFsrqxV","executionInfo":{"status":"error","timestamp":1674165774167,"user_tz":300,"elapsed":136,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"3fe5ccf2-95e2-48aa-f797-59b35ea42587"},"source":["\n","\n","nltk.download('wordnet')\n","\n","\n","\n","\n","\n","\n","\n","import unidecode\n","\n","\n","\n"],"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-eecb1e4b7e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"jan_f3OUL8fE"},"source":["## I. Text Processing in Python\n","\n","For text processing in Python, two popular libraries, namely NLTK (Natural Language Toolkit) and spaCy will be used in the tutorial."]},{"cell_type":"markdown","metadata":{"id":"Y3W3pcgAqseb"},"source":["For text processing we can perform a series of steps:\n","\n","1.  Remove symbols\n","2. Remove non-ASCII characters\n","3.    Remove extra whitespaces\n","5.    Expand contractions\n","6. Treatment for numbers\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0JH5N7ysG4OC","executionInfo":{"status":"ok","timestamp":1674165887669,"user_tz":300,"elapsed":134,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"source":["text = \"\"\"Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\"\"\""],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBjLgFbxx8Pb"},"source":["##Remove symbols\n","\n","A text may contain some unwanted symbols which will be a noise for our text analysis."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"EXqVO08gyRky","executionInfo":{"status":"ok","timestamp":1674165890755,"user_tz":300,"elapsed":136,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"e3d5e7cc-c8e3-41ba-edca-01fe4a8fb30f"},"source":["import numpy as np \n","\n","def remove_symbols(text):\n","    symbols = \"'\\<>?;:#@&()—\"\n","    for i in range(len(symbols)):\n","        text = np.char.replace(text, symbols[i], '')\n","    return str(text)\n","\n","\n","remove_symbols(\"Do we have extra #symbols in this @sentence. \")"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Do we have extra symbols in this sentence. '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["remove_symbols(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"s9260rm2hBLA","executionInfo":{"status":"ok","timestamp":1674165896208,"user_tz":300,"elapsed":143,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"ad1b3797-7c59-461b-f18f-ba7e788f9830"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Natural language processing NLP refers to the branch of computer scienceand more specifically, the branch of artificial intelligence or AIconcerned with giving computers the ability to understand text and spoken words in much the same way human beings can.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"zNEdOCca-KMa"},"source":["##Removing Non-ASCII characters\n","\n","ASCII represents lowercase letters (a-z), uppercase letters (A-Z), digits (0-9) and symbols such as punctuation marks. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CHDbgM4W-Ngy","executionInfo":{"status":"ok","timestamp":1674165906709,"user_tz":300,"elapsed":133,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"11d0cbcf-318c-4ef8-8891-da6c20d51584"},"source":["def remove_non_ascii(text):\n","  # encoding the text to ASCII format\n","  text_encode = text.encode(encoding=\"ascii\", errors=\"ignore\")\n","  # decoding the text\n","  text_decode = text_encode.decode()\n","  return text_decode\n","\n","remove_non_ascii(\"Python is easy \\u200c to learn\" )\n"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Python is easy  to learn'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ny7mMZG1jksQ","executionInfo":{"status":"ok","timestamp":1674165908114,"user_tz":300,"elapsed":123,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"fa2f2f4b-e3d2-4e95-f5fa-25dace5a3aee"},"source":["remove_non_ascii(\"àa string withé fuünny charactersß.\")"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'a string with funny characters.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["In the case of removing all the symbols and non-ASCII characters, one way is keeping only the characters and numbers in the text. "],"metadata":{"id":"8dj3-C0uZx8R"}},{"cell_type":"code","source":["import nltk #natual language toolkit\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","import re #regular expressions\n","\n","def keep_characters_numbers(text):\n","  # replace any text that is not characters and numbers\n","  cleaned_text = []\n","  for word in word_tokenize(text):\n","    cleaned_text.append(re.sub(\"[^a-zA-Z0-9]\", \"\", word))\n","  return \" \".join(cleaned_text)"],"metadata":{"id":"dPFWiRIxZwhZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674165911249,"user_tz":300,"elapsed":1467,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"f16f5950-b9ea-4b72-9994-341cdd36ecdd"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["keep_characters_numbers(\"àa string withé fuünny charactersß. The number is 2012.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"F6SkF7PDa5Tm","executionInfo":{"status":"ok","timestamp":1674165912340,"user_tz":300,"elapsed":5,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"b292af06-bae8-44fc-acaa-693daedfb3ef"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'a string with funny characters  The number is 2012 '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"0onKKJe5xH2w"},"source":["## Remove extra whitespaces\n","Sometimes there are extra white spaces in the text which are necessary to be removed"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZ4LSnVLxokk","executionInfo":{"status":"ok","timestamp":1674165914153,"user_tz":300,"elapsed":147,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"44066808-c0d1-4db9-b79f-621ede473ce5"},"source":["\n","def remove_whitespace(text):\n","    \"\"\"remove extra whitespaces from text\"\"\"\n","    text = text.strip()  #removes any leading (spaces at the beginning) and trailing (spaces at the end) characters \n","    return \" \".join(text.split())\n","    \n","print(remove_whitespace(\"   Here, there are extra    white     spaces  \"))\n"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Here, there are extra white spaces\n"]}]},{"cell_type":"markdown","metadata":{"id":"jDYIDqUtssQv"},"source":["##Expand Contractions\n","\n","Contractions are shortened words, e.g., don’t and can’t. Expanding such words to “do not” and “can not” helps to standardize text.\n","\n","We use the contractions module to expand the contractions."]},{"cell_type":"code","metadata":{"id":"BUgnUTaqslVr","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1674165916709,"user_tz":300,"elapsed":127,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"821b0778-71d3-43af-ce54-bc43007a1918"},"source":["import contractions\n","\n","def expand_contractions(text):\n","    text = contractions.fix(text)\n","    return text\n","\n","expand_contractions(\"\"\"expand shortened words, e.g. don't to do not\"\"\")"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'expand shortened words, e.g. do not to do not'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"o0SdoqcHs9ie"},"source":["*Note: This step is optional depending on your NLP task as spaCy’s tokenization and lemmatization functions will perform the same effect to expand contractions such as can’t and don’t. The slight difference is that spaCy will expand “we’re” to “we be” while pycontractions will give result “we are”.*"]},{"cell_type":"markdown","metadata":{"id":"pnUi04rktLLV"},"source":["##Treatment for Numbers\n","\n","There are two steps in our treatment of numbers.\n","\n","One of the steps involve the conversion of number words to numeric form, e.g., seven to 7, to standardize text. To do this, we use the word2number module. Sample code as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wt7q4pBctR0f","executionInfo":{"status":"ok","timestamp":1674165925811,"user_tz":300,"elapsed":6388,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"690a283b-9574-4b66-f3e4-e6ae71f7a0ca"},"source":["import spacy\n","# load spacy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","from word2number import w2n\n","\n","text = \"\"\"three cups of coffee\"\"\"\n","doc = nlp(text) #create a doc object\n","tokens = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' else token for token in doc]\n","\n","print(tokens) "],"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]},{"output_type":"stream","name":"stdout","text":["[3, cups, of, coffee]\n"]}]},{"cell_type":"markdown","metadata":{"id":"svDGFXXCtN8b"},"source":["\n","The other step is to remove numbers. Removing numbers may make sense for sentiment analysis since numbers contain no information about sentiments. However, if our NLP task is to extract the number of tickets ordered in a message to our chatbot, we will definitely not want to remove numbers."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HtgVnVFH3M3O","executionInfo":{"status":"ok","timestamp":1674165965472,"user_tz":300,"elapsed":163,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"0aba7561-4dcc-4a0d-e1d5-2e65841c76df"},"source":["def remove_numbers(text):\n","  text = re.sub(r\" \\d\", \"\", text)\n","  return str(text)\n","\n","remove_numbers(\"Here, the is number 7 that we don't need.\")"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Here, the is number that we don't need.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"UvJVNu-Cs019"},"source":["## II. Text Normalization\n","\n","Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”."]},{"cell_type":"markdown","metadata":{"id":"Tqs9zQ2A0Z7g"},"source":["1. Lowercase all texts\n","2. Remove stopwords\n","3. Lemmatization\n","4. Stemming"]},{"cell_type":"markdown","metadata":{"id":"NDiKlHw45bGi"},"source":["## Lower case the text\n","\n","Converting all your data to lowercase helps in the process of preprocessing and in later stages in the NLP application, when you are doing parsing."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"gVQxKViQszq3","executionInfo":{"status":"ok","timestamp":1674165968032,"user_tz":300,"elapsed":120,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"ba2cc3fe-8316-4723-dcc5-daab29763a8c"},"source":["def convert_lower_case(text):\n","    return str(np.char.lower(text))\n","\n","convert_lower_case(\"THIS is An ExaMple to Convert a Text To a lower CASE.\")"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'this is an example to convert a text to a lower case.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"K41jif9R6IgP"},"source":["## Remove Stop words\n","\n","Stopwords are referring to words that do not carry much insight, such as prepositions. NLTK and spaCy have different amounts of stopwords in the library, but both NLTK and spaCy allowed us to add in any word we feel necessary. For example, when we deal with email, we may add Gmail, com, outlook as stopwords."]},{"cell_type":"code","metadata":{"id":"iteW0Vgo6FSM"},"source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Get the list of stop words\n","stop_words = stopwords.words('english')\n","# add new stopwords to the list\n","stop_words.extend([\"could\",\"though\",\"would\",\"also\",\"many\",'much'])\n","print(stop_words)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgMhuf2S6hIU","executionInfo":{"status":"ok","timestamp":1674165972745,"user_tz":300,"elapsed":126,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"18b489d5-bfb3-4ad4-b152-2f266f6260ef"},"source":["text = \"The idea of giving computers the ability to process human language is as old as the idea of computers themselves. \"\n","\n","# Remove the stopwords from the list of tokens\n","tokens = [x for x in  word_tokenize(convert_lower_case(text)) if x not in stop_words]\n","print(tokens)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["['idea', 'giving', 'computers', 'ability', 'process', 'human', 'language', 'old', 'idea', 'computers', '.']\n"]}]},{"cell_type":"markdown","metadata":{"id":"urIGnwnn8am-"},"source":["## Lemmatization\n","\n","Lemmatization is the process of converting a word to its base form, e.g., “caring” to “care”. We use spaCy’s lemmatizer to obtain the lemma, or base form, of the words. Sample code:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfW4IhhG6w_6","executionInfo":{"status":"ok","timestamp":1674165975691,"user_tz":300,"elapsed":141,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"99cd69e2-c73c-4898-eeea-de48059afda1"},"source":["text = \"\"\"I'm happiness commitment running are\"\"\"\n","doc = nlp(text) #create a doc object\n","mytokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc] # -PRON- is the default lemma for pronouns in spaCy\n","print(mytokens) "],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'be', 'happiness', 'commitment', 'running', 'be']\n"]}]},{"cell_type":"markdown","source":["## Stemming\n","Stemming is similar to lemmatization with the difference that in lemma word is actual word, but stem word can be a word without meaning."],"metadata":{"id":"3hFU8FFx9l6x"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","def stemming(text):\n","  token_words=word_tokenize(text)\n","  stem_sentence=[]\n","  for word in token_words:\n","      stem_sentence.append(porter.stem(word))\n","  return stem_sentence"],"metadata":{"id":"GMTGIqgfGh0Z","executionInfo":{"status":"ok","timestamp":1674165938341,"user_tz":300,"elapsed":124,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print(stemming(text)) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nmzNqdcIgoE","executionInfo":{"status":"ok","timestamp":1674165978754,"user_tz":300,"elapsed":127,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"87c990db-00a5-4e5b-a3bb-115a45e04378"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', \"'m\", 'happi', 'commit', 'run', 'are']\n"]}]},{"cell_type":"code","source":["stemming('happy')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJzVCDcoJAe7","executionInfo":{"status":"ok","timestamp":1674165979554,"user_tz":300,"elapsed":152,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"32460a75-8af1-40ac-d45e-57ab1d9cee1f"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['happi']"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"wp1gnSW48w9J"},"source":["##**Assignment:**\n","\n","Putting everything together, the full text preprocessing code (define a function as text_preprocessing):\n","\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"09t57J-l9k7e"}},{"cell_type":"code","metadata":{"id":"mjcwUloR8h4Q","executionInfo":{"status":"ok","timestamp":1674166857611,"user_tz":300,"elapsed":126,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"source":["text = \"I'd like to have three cups   of coffee #?> Your Café is #awesome.\"\n","\n","def text_preprocessing(text):\n","  text = keep_characters_numbers(remove_symbols(text))\n","  return str(text)\n","\n","\n","\n","# result: I would like to have three cups of coffee? Your Cafe is awsome."],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"zOv4QDFMJie8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1674166866960,"user_tz":300,"elapsed":140,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"ffbe502d-66fe-49c4-858a-5c9461dff471"},"source":["text_preprocessing(text)"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Id like to have three cups of coffee Your Caf is awesome '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"Hxg22ZGTFXhi"},"source":["Put all text normalization function together!"]},{"cell_type":"code","metadata":{"id":"x9tMM3P2FnH0"},"source":["def text_normalization(text):\n","\n","\n","#result: ['like', 'cup', 'coffee', 'cafe', 'delicious']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3eXibEIg9Z56"},"source":["Refrences:\n","\n","https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n","\n","https://towardsdatascience.com/text-processing-in-python-29e86ea4114c"]}]}
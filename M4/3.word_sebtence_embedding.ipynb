{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Word\\Sentence Embedding:"],"metadata":{"id":"GENdAAb1Aps8"}},{"cell_type":"markdown","source":["We start this section by introducing Word2Vec embedding and how you can use Gensim to train a Word2vec model on a list of sentences. As we described in the slides, the Word2vec algorithms are skip-gram and CBOW. In the following, we show how we train a model for either of them. For more information about Word2vec embedding and Gensim, please visit [here](https://rare-technologies.com/word2vec-tutorial/). "],"metadata":{"id":"WvqoaOBVIUi1"}},{"cell_type":"markdown","source":["In this tutorial, you’ll learn:\n","\n","    How to use Gensim to train a Word2Vec embedding model\n","    How to upload pretrained models\n","    How to train BOW and TF-IDF models"],"metadata":{"id":"4Bw6-o-4oOjW"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"eRuyhAuBY_8N","executionInfo":{"status":"ok","timestamp":1674226717647,"user_tz":300,"elapsed":122,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"_0PBRSqGueZv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674226718411,"user_tz":300,"elapsed":129,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"71b4b11f-1cd9-4a16-be1d-15c531910487"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}],"source":["# Python program to generate word vectors using Word2Vec\n","  \n","# importing all necessary modules\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","  \n","import gensim\n","from gensim.models import Word2Vec\n","import nltk  \n","nltk.download('punkt')"]},{"cell_type":"markdown","source":["##Word2vec Embedding\n"],"metadata":{"id":"60koC6R_Jmbl"}},{"cell_type":"code","source":["sentences = 'the car is owned by my father. the house is owned by my mother.'\n","  \n","data = []\n","  \n","# iterate through each sentence in the file\n","for i in sent_tokenize(sentences):\n","    temp = []\n","      \n","    # tokenize the sentence into words\n","    for j in word_tokenize(i):\n","        temp.append(j.lower())\n","  \n","    data.append(temp)"],"metadata":{"id":"lj2kSMd1fhyz","executionInfo":{"status":"ok","timestamp":1674226720210,"user_tz":300,"elapsed":140,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIVW3FhMJ3dq","executionInfo":{"status":"ok","timestamp":1674226722133,"user_tz":300,"elapsed":283,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"edffb828-916c-498a-f128-0588564bbe6b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[['the', 'car', 'is', 'owned', 'by', 'my', 'father', '.'], ['the', 'house', 'is', 'owned', 'by', 'my', 'mother', '.']]\n"]}]},{"cell_type":"markdown","source":["## models.word2vec()"],"metadata":{"id":"qi_4FMYTKHZN"}},{"cell_type":"markdown","source":["\n","\n","```\n","class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, \n","vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, \n","sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, \n","ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, \n","null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, \n","compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, \n","shrink_windows=True)\n","```\n","**sentences** (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens\n","\n","**vector_size** (int, optional) – Dimensionality of the word vectors\n","\n","**window** (int, optional) – Maximum distance between the current and predicted word within a sentence\n","\n","**min_count** (int, optional) – Ignores all words with total frequency lower than this\n","\n","**sg** ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW\n","\n","more information: https://radimrehurek.com/gensim/models/word2vec.html"],"metadata":{"id":"PnBVHdohPcUf"}},{"cell_type":"code","source":["# Create CBOW model\n","model1 = gensim.models.Word2Vec(data, min_count = 1, \n","                              size = 10, window = 2)\n"],"metadata":{"id":"u21vA7QAKA9-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674226723980,"user_tz":300,"elapsed":132,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"9b2e7c3d-d04d-4be4-d951-b20410742cdc"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n","WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"]}]},{"cell_type":"code","source":["# you can save the model\n","model1.save(\"word2vec.model\")\n","\n","# and load it\n","model1 = Word2Vec.load(\"word2vec.model\")"],"metadata":{"id":"dh0PIOPTRCD2","executionInfo":{"status":"ok","timestamp":1674226725506,"user_tz":300,"elapsed":157,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# reach out the vector of a word\n","model1.wv['house']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BjRdH0QRtWu","executionInfo":{"status":"ok","timestamp":1674226726600,"user_tz":300,"elapsed":111,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"0b4ad293-4748-497e-df29-5e945bb8f8b6"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.00257788,  0.03419786,  0.00961709,  0.03859387, -0.01584929,\n","        0.01011667, -0.04816683, -0.01648258, -0.01717688,  0.02342607],\n","      dtype=float32)"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["model1.wv.most_similar('owned', topn=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tE_uaKrDRmHr","executionInfo":{"status":"ok","timestamp":1674226728586,"user_tz":300,"elapsed":113,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"c1254687-d37b-4848-96c7-45a712c7fb7b"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('car', 0.29851916432380676),\n"," ('house', 0.22156289219856262),\n"," ('is', 0.12342262268066406),\n"," ('the', 0.10404221713542938),\n"," ('father', 0.08825638890266418),\n"," ('by', 0.08316656947135925),\n"," ('my', -0.001071631908416748),\n"," ('mother', -0.07891765236854553),\n"," ('.', -0.48244214057922363)]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Print results\n","print(\"Cosine similarity between 'mother' \" + \n","               \"and 'father' - CBOW : \",\n","    model1.similarity('mother', 'father'))\n","      \n","print(\"Cosine similarity between 'house' \" +\n","                 \"and 'car' - CBOW : \",\n","      model1.similarity('house', 'car'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCbkcAkYRhV7","executionInfo":{"status":"ok","timestamp":1674226730663,"user_tz":300,"elapsed":164,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"2ca9a9b2-0b49-4f7d-fecc-9fe5881a78f2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between 'mother' and 'father' - CBOW :  0.33359298\n","Cosine similarity between 'house' and 'car' - CBOW :  0.23999052\n"]}]},{"cell_type":"code","source":["# Create Skip Gram model\n","model2 = gensim.models.Word2Vec(data, min_count = 1, size = 10,\n","                                             window = 3, sg = 1)\n","  \n"],"metadata":{"id":"bSC3ytPsKEb3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674226661893,"user_tz":300,"elapsed":128,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"4ef8fbcc-820f-45b8-e397-85e22941d4ac"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n","WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"]}]},{"cell_type":"code","source":["# reach out the vector of a word\n","model2.wv['house']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63RXkc35SviQ","executionInfo":{"status":"ok","timestamp":1674226668721,"user_tz":300,"elapsed":123,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"088b07af-7275-417f-b67d-5c5c4777eba0"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.00258364,  0.0342035 ,  0.00961899,  0.03859608, -0.01584661,\n","        0.0101135 , -0.04817257, -0.0164755 , -0.01717148,  0.02342499],\n","      dtype=float32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["model2.wv.most_similar('owned', topn=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYV9C_bkS4nC","executionInfo":{"status":"ok","timestamp":1674226672707,"user_tz":300,"elapsed":152,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"589b41e9-daa9-43a8-9d88-4d06bb626918"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('car', 0.29851916432380676),\n"," ('house', 0.221551313996315),\n"," ('is', 0.12342262268066406),\n"," ('the', 0.10404221713542938),\n"," ('father', 0.08824722468852997),\n"," ('by', 0.08316656947135925),\n"," ('my', -0.001071631908416748),\n"," ('mother', -0.07891765236854553),\n"," ('.', -0.48245689272880554)]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Print results\n","print(\"Cosine similarity between 'mother' \" +\n","          \"and 'father' - Skip Gram : \",\n","    model2.similarity('mother', 'father'))\n","      \n","print(\"Cosine similarity between 'house' \" +\n","            \"and 'car' - Skip Gram : \",\n","      model2.similarity('house', 'car'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WXDvWjbSyCO","executionInfo":{"status":"ok","timestamp":1674226681395,"user_tz":300,"elapsed":123,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"31c71d9d-ad31-460e-b941-c1ac5a5d4d83"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between 'mother' and 'father' - Skip Gram :  0.33357447\n","Cosine similarity between 'house' and 'car' - Skip Gram :  0.24001113\n"]}]},{"cell_type":"markdown","source":["## Pretrained Models"],"metadata":{"id":"1YXPkKAyXWXc"}},{"cell_type":"code","source":["import gensim.downloader\n","print(list(gensim.downloader.info()['models'].keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4JWuo1wXX0K","executionInfo":{"status":"ok","timestamp":1674226819431,"user_tz":300,"elapsed":287,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"bc6e2977-4f54-44e4-8b7f-1f6f0e621992"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"]}]},{"cell_type":"code","source":["glove_vectors = gensim.downloader.load('glove-twitter-25')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gz8KQGB7X2lS","executionInfo":{"status":"ok","timestamp":1674226881896,"user_tz":300,"elapsed":60069,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"8f03e34f-0dcf-40c7-b86d-2b3541f76ab9"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 104.8/104.8MB downloaded\n"]}]},{"cell_type":"code","source":["glove_vectors.most_similar('twitter')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwC151W9X6Rl","executionInfo":{"status":"ok","timestamp":1674226886788,"user_tz":300,"elapsed":472,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"099fee38-61b6-476f-a717-4e995d2e4de3"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('facebook', 0.9480051398277283),\n"," ('tweet', 0.9403422474861145),\n"," ('fb', 0.9342358708381653),\n"," ('instagram', 0.9104823470115662),\n"," ('chat', 0.8964964747428894),\n"," ('hashtag', 0.8885936141014099),\n"," ('tweets', 0.8878157734870911),\n"," ('tl', 0.8778461813926697),\n"," ('link', 0.877821147441864),\n"," ('internet', 0.8753897547721863)]"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Word Embedding with TF-IDF and BOW"],"metadata":{"id":"rv2FENOvTPpw"}},{"cell_type":"code","source":["# TfidfVectorizer \n","# CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","import pandas as pd\n"],"metadata":{"id":"Do37WFRpkyAI","executionInfo":{"status":"ok","timestamp":1674226937609,"user_tz":300,"elapsed":415,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# set of documents\n","train =  sent_tokenize(sentences)\n","# instantiate the vectorizer object\n","countvectorizer = CountVectorizer(analyzer= 'word')\n","tfidfvectorizer = TfidfVectorizer()\n","# convert th documents into a matrix\n","count_wm = countvectorizer.fit_transform(train)\n","tfidf_wm = tfidfvectorizer.fit_transform(train)\n"],"metadata":{"id":"ksUvnViJV9a6","executionInfo":{"status":"ok","timestamp":1674226939353,"user_tz":300,"elapsed":110,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["#retrieve the terms found in the corpora\n","# if we take same parameters for both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n","#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n","count_tokens = countvectorizer.get_feature_names()\n","tfidf_tokens = tfidfvectorizer.get_feature_names()\n"],"metadata":{"id":"Uk8qnJOeWCeB","executionInfo":{"status":"ok","timestamp":1674226941816,"user_tz":300,"elapsed":121,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\n","df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\n"],"metadata":{"id":"Ix2IwqvJWEK3","executionInfo":{"status":"ok","timestamp":1674226992131,"user_tz":300,"elapsed":117,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["print(\"Count Vectorizer\\n\")\n","print(df_countvect)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIapkuk5WHGC","executionInfo":{"status":"ok","timestamp":1674226993559,"user_tz":300,"elapsed":144,"user":{"displayName":"Dominique Monnet","userId":"07064807012742912194"}},"outputId":"a88d5354-726c-4bf7-8ea2-b9b3d094c714"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Count Vectorizer\n","\n","      by  car  father  house  is  mother  my  owned  the\n","Doc1   1    1       1      0   1       0   1      1    1\n","Doc2   1    0       0      1   1       1   1      1    1\n"]}]},{"cell_type":"code","source":["df_tfidfvect.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"tsCGTG3DWLQa","executionInfo":{"status":"ok","timestamp":1643217493629,"user_tz":300,"elapsed":166,"user":{"displayName":"Elham Kh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03387393347125994499"}},"outputId":"c0f01b76-10d1-42be-e769-79caa0c5de40"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-fda8b88b-47ec-4c02-b4cf-82e503fca570\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>by</th>\n","      <th>car</th>\n","      <th>father</th>\n","      <th>house</th>\n","      <th>is</th>\n","      <th>mother</th>\n","      <th>my</th>\n","      <th>owned</th>\n","      <th>the</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Doc1</th>\n","      <td>0.334251</td>\n","      <td>0.469778</td>\n","      <td>0.469778</td>\n","      <td>0.000000</td>\n","      <td>0.334251</td>\n","      <td>0.000000</td>\n","      <td>0.334251</td>\n","      <td>0.334251</td>\n","      <td>0.334251</td>\n","    </tr>\n","    <tr>\n","      <th>Doc2</th>\n","      <td>0.334251</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.469778</td>\n","      <td>0.334251</td>\n","      <td>0.469778</td>\n","      <td>0.334251</td>\n","      <td>0.334251</td>\n","      <td>0.334251</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fda8b88b-47ec-4c02-b4cf-82e503fca570')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fda8b88b-47ec-4c02-b4cf-82e503fca570 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fda8b88b-47ec-4c02-b4cf-82e503fca570');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["            by       car    father  ...        my     owned       the\n","Doc1  0.334251  0.469778  0.469778  ...  0.334251  0.334251  0.334251\n","Doc2  0.334251  0.000000  0.000000  ...  0.334251  0.334251  0.334251\n","\n","[2 rows x 9 columns]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["As you can see, the value of tfidfvectorizer from sklearn is different from what we manually computed in the slides. Let's see what the reason is:\n","\n","Based on [scikit-learn website](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html), `tf-idf` of a term t in doucment d is computed as `tf-idf(t, d) = tf(t, d) * idf(t)`. The value of `tf(t, d)` is the same as we defined. However, they add 1 to the value of `idf(t)` because they want to not ignor the words that apear in all documents (sentences). If the variable `smooth_idf` is `True` in `sklearn.feature_extraction.text.TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)`then the formula for `idf(t)` will be `idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1 `( if `smooth_idf` is `False` the formula will be `idf(t) = log [ n / (df(t) + 1) ])`.). The final vector for document `d` is normalized based on the norm chosen in `norm` and the default is `l2` and can be `l1` as well. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for more details. \n"],"metadata":{"id":"TviVC7XHY8qq"}},{"cell_type":"markdown","source":["# Assignment\n","Take the data we created in the Data Collection tutorial, and try to train the word and sentence embedding models you learned in the current tutorial."],"metadata":{"id":"yksyKdesnUvA"}}]}